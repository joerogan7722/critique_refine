{
  "timestamp": "2025-06-27T10:16:16.802799",
  "original_user_prompt": "",
  "context_documents": [],
  "initial_generation": {
    "text": "\n\n---\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            elif self.default_critic_role_prompt_file:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n            else:\n                raise ValueError(\"No critic role specified in the configuration.\")\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i + 1}\"\n                return current_text\n            except TemplateNotFoundError:\n                raise  # Re-raise to be caught by the main run loop's handler\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i + 1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model,\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if isinstance(response, str):\n                try:\n                    return json.loads(response)\n                except json.JSONDecodeError:\n                    # Handle cases where the string is not valid JSON\n                    return {\"actionable\": \"ACTIONABLE\" in response}\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except TemplateNotFoundError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Template not found, stopping loop: {e}\", exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = f\"Template not found: {e}\"\n            self.run_log[\"final_output\"] = \"\"\n            self.logger.log_run(self.run_log)\n            raise  # Re-raise the specific error to be caught by the caller\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n",
    "model_used": "N/A (provided content for review)"
  },
  "critiques": [
    {
      "round": 1,
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": "critic.txt",
      "text": "This code is well-structured and generally well-written, showing a good understanding of asynchronous programming and error handling. However, several areas could be improved for clarity, conciseness, and robustness.\n\n**Actionable Critique:**\n\n1. **Redundant Configuration Access:** The `__init__` method repeatedly accesses attributes from `self.run_config` and its nested objects.  This makes the constructor lengthy and less readable. Consider creating a helper function to unpack the configuration once and store relevant attributes directly.  This reduces redundancy and improves readability.\n\n2. **Log Formatting:** The log messages using `logging.info` often include `f\"[{datetime.now().isoformat()}]\"`. This can be centralized to improve consistency and reduce code duplication. Consider creating a logging function that handles timestamping.  It would make the logs cleaner.\n\n3. **Error Handling in `_get_critique`:**  The `_get_critique` function catches `ModelCallError` and `Exception` separately, but both are logged with `logging.exception`. This might mask important differences in error types. Consider more specific error handling or at least distinct log messages for different exception types.\n\n4. **`_is_critique_actionable` Logic:** The `_is_critique_actionable` function handles various return types from the meta-critic model (string, dict) with different logic. This is complex and error-prone. Ensure that the meta-critic model always returns a consistent structured output (e.g., a dictionary with an \"actionable\" key) to simplify this logic.  The handling of string responses that might contain \"ACTIONABLE\" is brittle.  The model should return a proper JSON response.\n\n5. **Exception Handling in `run`:** The `run` method catches `TemplateNotFoundError` and then re-raises it. While this allows the caller to handle this specific error, it's less efficient to re-raise and re-catch. Consider refining the error handling in the caller instead. The `except Exception` block is overly broad; it should handle more specific potential errors.\n\n6. **`_refine` Role Prompt:** The `_refine` function uses `self.run_config.roles.get(\"refiner_template\")`. The code lacks handling for the case where this template is not found, potentially leading to a runtime failure. Add error handling for this scenario, similar to how you handle missing templates in other places.\n\n\n7. **Clarity in `_run_critique_refine_loop`:** The logic for stopping based on `self.stop_threshold` is unclear. The `_is_critique_actionable` function is called, but the result is not used in this function.  If it is intended to stop when the critique is not actionable, the loop should break when `actionability` is `False`.\n\n8. **`_generate` Function:** The logic for handling `initial_content_for_review` is somewhat implicit.  Consider making the logic clearer and potentially separating this into two functions.\n\n**Specific Code Suggestions:**\n\n* Replace the verbose `__init__` with a more concise method leveraging parameter unpacking or a helper function.\n* Create a custom logging function to centralize timestamping and formatting.\n* Refactor `_is_critique_actionable` to handle only dictionary responses from the meta-critic model and ensure that the model consistently produces these responses.\n* Add explicit handling for missing templates and configuration values.\n* Improve the logging messages to be more informative and precise.\n\nBy addressing these points, you'll create a more robust, maintainable, and readable codebase.  The core logic is sound, but these refinements significantly enhance its quality.\n"
    }
  ],
  "refinements": [],
  "reason_for_stopping": "Non-actionable critique received in round 1.",
  "final_output": "\n\n---\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            elif self.default_critic_role_prompt_file:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n            else:\n                raise ValueError(\"No critic role specified in the configuration.\")\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i + 1}\"\n                return current_text\n            except TemplateNotFoundError:\n                raise  # Re-raise to be caught by the main run loop's handler\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i + 1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model,\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if isinstance(response, str):\n                try:\n                    return json.loads(response)\n                except json.JSONDecodeError:\n                    # Handle cases where the string is not valid JSON\n                    return {\"actionable\": \"ACTIONABLE\" in response}\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except TemplateNotFoundError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Template not found, stopping loop: {e}\", exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = f\"Template not found: {e}\"\n            self.run_log[\"final_output\"] = \"\"\n            self.logger.log_run(self.run_log)\n            raise  # Re-raise the specific error to be caught by the caller\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n",
  "config_used": {
    "generator_model": "gemini-1.5-flash",
    "critic_model": "gemini-1.5-flash",
    "refiner_model": "gemini-1.5-flash",
    "max_rounds": 3,
    "stop_on_no_actionable_critique_threshold": 50,
    "default_critic_role_prompt_file": "critic.txt",
    "default_refiner_role_prompt_file": "refiner.txt",
    "multi_critic_roles": null,
    "disable_meta_critic": false
  },
  "runtime": 6.047544717788696
}