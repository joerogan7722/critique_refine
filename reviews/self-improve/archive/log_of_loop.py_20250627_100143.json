{
  "timestamp": "2025-06-27T10:01:36.937774",
  "original_user_prompt": "",
  "context_documents": [],
  "initial_generation": {
    "text": "\n\n---\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            elif self.default_critic_role_prompt_file:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n            else:\n                raise ValueError(\"No critic role specified in the configuration.\")\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i + 1}\"\n                return current_text\n            except TemplateNotFoundError:\n                raise  # Re-raise to be caught by the main run loop's handler\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i + 1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model,\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except TemplateNotFoundError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Template not found, stopping loop: {e}\", exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = f\"Template not found: {e}\"\n            self.run_log[\"final_output\"] = \"\"\n            self.logger.log_run(self.run_log)\n            raise  # Re-raise the specific error to be caught by the caller\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n",
    "model_used": "N/A (provided content for review)"
  },
  "critiques": [
    {
      "round": 1,
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": "critic.txt",
      "text": "This code is well-structured and generally well-written, but several improvements can enhance clarity, conciseness, and error handling.\n\n**Clarity and Conciseness:**\n\n* **`__init__` method:**  The `__init__` method is excessively long.  Consider creating a helper function to handle the configuration loading and logging setup. This improves readability and maintainability.\n\n* **Redundant Logging:** The logging within `_generate`, `_critique`, and `_refine` could be consolidated.  Instead of logging the entire response, log a summary (e.g., length, first few lines) to avoid cluttering the log file with massive outputs.\n\n* **Variable Names:**  Some variable names are a bit verbose (e.g., `initial_content_for_review`). Consider shorter, more descriptive names where possible.\n\n* **Docstrings:** While docstrings are present, they could be more precise and informative. For example, the `_generate` docstring could specify the return type more accurately (including the dictionary's contents).\n\n**Accuracy and Completeness:**\n\n* **Error Handling:** While the code handles some exceptions,  it could be more robust. For instance,  the `_get_critique` function catches `ModelCallError` and other Exceptions separately, but both lead to logging the error.  Combine the exception handling to avoid duplication.  Consider adding more specific exception handling based on the potential errors from `call_model`.\n\n* **Meta-Critic Handling:** The `_is_critique_actionable` function assumes the meta-critique response always contains an \"actionable\" key.  This should be validated to prevent unexpected behavior if the model returns a different format.  Add explicit checks for the existence of keys before accessing them.\n\n* **`call_model` function:**  The code relies on a `call_model` function defined elsewhere.  Providing a stub or a brief explanation of this function would improve understanding.  Consider its potential failure points and how they impact this class.\n\n**Logical Flow and Structure:**\n\n* **`_run_critique_refine_loop`:** The exception handling in this function could be simplified.  The repetitive structure for different exception types can be streamlined using a single `except Exception` block with more detailed logging.\n\n* **Separation of Concerns:**  The logging logic is tightly coupled with the main logic. Separating logging into a dedicated logging class or a helper function would make the code cleaner.\n\n**Tone and Style:**\n\n* The code's tone is generally good, but consistency in logging messages (e.g., timestamp formatting) should be maintained.\n\n**Grammar and Spelling Errors:** No issues detected.\n\n\n**Actionable Critique:**\n\n1. **Refactor `__init__`:** Create a helper function to handle configuration loading and logger initialization.\n\n2. **Consolidate Logging:** Reduce verbose logging in the model-calling functions; log summaries instead of the full output.\n\n3. **Improve Error Handling:** Consolidate exception handling in `_get_critique`. Add more specific exception handling where appropriate, including checks for unexpected responses from `call_model` and `_get_meta_critique`.\n\n4. **Enhance Meta-Critic Handling:**  Add explicit checks for the existence of keys in the meta-critique response before accessing them.\n\n5. **Streamline Exception Handling in `_run_critique_refine_loop`:** Use a single `except Exception` block with more detailed logging to avoid repetition.\n\n6. **Document `call_model`:**  Either provide a stub implementation or a clear explanation of its functionality and potential failure modes.\n\n7. **Improve Docstrings:** Make docstrings more precise and informative about return types and potential exceptions.\n\n8. **Consider a dedicated logging class:** This would improve the separation of concerns and make logging more maintainable.\n\n\nBy addressing these points, the code will be significantly improved in terms of readability, maintainability, and robustness.\n"
    }
  ],
  "refinements": [],
  "reason_for_stopping": "Non-actionable critique received in round 1.",
  "final_output": "\n\n---\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            elif self.default_critic_role_prompt_file:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n            else:\n                raise ValueError(\"No critic role specified in the configuration.\")\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i + 1}\"\n                return current_text\n            except TemplateNotFoundError:\n                raise  # Re-raise to be caught by the main run loop's handler\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i + 1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model,\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except TemplateNotFoundError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Template not found, stopping loop: {e}\", exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = f\"Template not found: {e}\"\n            self.run_log[\"final_output\"] = \"\"\n            self.logger.log_run(self.run_log)\n            raise  # Re-raise the specific error to be caught by the caller\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n",
  "config_used": {
    "generator_model": "gemini-1.5-flash",
    "critic_model": "gemini-1.5-flash",
    "refiner_model": "gemini-1.5-flash",
    "max_rounds": 3,
    "stop_on_no_actionable_critique_threshold": 50,
    "default_critic_role_prompt_file": "critic.txt",
    "default_refiner_role_prompt_file": "refiner.txt",
    "multi_critic_roles": null,
    "disable_meta_critic": false
  },
  "runtime": 6.258560657501221
}