{
  "timestamp": "2025-06-27T09:35:59.192595",
  "original_user_prompt": "",
  "context_documents": [],
  "initial_generation": {
    "text": "\n\n---\n\nimport asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport argparse\n\nfrom core.loop import CritiqueRefineLoop\nfrom utils.config import (\n    get_project_context_path,\n    build_run_config,\n    get_logging_config,\n)\n\n\nclass SelfReviewTool:\n    \"\"\"A tool for self-reviewing code using a critique-refine loop.\"\"\"\n\n    def __init__(self, full_config: Dict[str, Any], output_dir: Optional[Path] = None):\n        \"\"\"Initialize the SelfReviewTool.\n\n        Args:\n            full_config: The full application configuration.\n            output_dir: The directory to save review outputs. Defaults to 'reviews/self-improve'.\n        \"\"\"\n        self.full_config = full_config\n        self.logging_config = get_logging_config()\n        if output_dir is None:\n            self.output_dir = Path(\"reviews\") / \"self-improve\"\n        else:\n            self.output_dir = output_dir\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    async def _load_file_content(self, file_path: Path) -> str:\n        \"\"\"Load the content of a file, prepending project context if available.\n\n        Args:\n            file_path: The path to the file to load.\n\n        Returns:\n            The file content with project context prepended.\n\n        Raises:\n            FileNotFoundError: If the target file does not exist.\n        \"\"\"\n        logging.info(\"Loading file content from: %s\", file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Target file not found: {file_path}\")\n\n        content_to_review = file_path.read_text(encoding=\"utf-8\")\n\n        project_context_path = get_project_context_path()\n        project_context = \"\"\n        if project_context_path and project_context_path.exists():\n            project_context = project_context_path.read_text(encoding=\"utf-8\")\n            logging.info(\"Loaded Project Context from %s\", project_context_path)\n        else:\n            logging.warning(\n                \"Project context file not found at %s. Proceeding without context.\",\n                project_context_path,\n            )\n\n        # Prepend project context to the content to be reviewed\n        full_content_to_review = f\"{project_context}\\n\\n---\\n\\n{content_to_review}\"\n        return full_content_to_review\n\n    async def _run_review_loop(\n        self, initial_content: str, args: Any, dry_run: bool\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Run the critique-refine loop with the given content and arguments.\n\n        Args:\n            initial_content: The content to be reviewed.\n            args: Command-line arguments to configure the run.\n            dry_run: If True, simulates the loop without actual model calls.\n\n        Returns:\n            A tuple containing the final refined output and the run log.\n        \"\"\"\n        logging.info(\"Starting critique-refine loop for self-review.\")\n        run_config = build_run_config(args, self.full_config)\n        run_config.dry_run = dry_run\n\n        loop = CritiqueRefineLoop(run_config)\n        final_output, run_log = await loop.run(\n            initial_user_prompt=\"\",  # Not used in self-review mode\n            initial_content_for_review=initial_content,\n        )\n        return final_output, run_log\n\n    async def _save_output(\n        self,\n        original_file_path: Path,\n        final_output: str,\n        run_log: Dict[str, Any],\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Save the review results to disk.\n\n        This saves a Markdown review, a structured JSON log, and optionally a\n        suggested code improvement file.\n\n        Args:\n            original_file_path: Path to the original file that was reviewed.\n            final_output: The final refined text from the loop.\n            run_log: The structured log of the critique-refine run.\n            save_improvement: Whether to save the suggested improvement to a file.\n\n        Returns:\n            A list of strings describing the results of the save operations.\n        \"\"\"\n        results = []\n        current_timestamp = datetime.now()\n\n        # Save Markdown review\n        output_filename = (\n            f\"review_of_{original_file_path.name}_\"\n            f\"{current_timestamp.strftime('%Y%m%d_%H%M%S')}.md\"\n        )\n        output_filepath = self.output_dir / output_filename\n        try:\n            with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(f\"# Self-Review of {original_file_path}\\n\\n\")\n                f.write(f\"**Timestamp:** {current_timestamp.isoformat()}\\n\\n\")\n                f.write(\n                    f\"**Original Content:**\\n```\\n\"\n                    f\"{original_file_path.read_text(encoding='utf-8')}\\n```\\n\\n\"\n                )\n                f.write(f\"**Critique-Refine Output:**\\n{final_output}\\n\")\n            results.append(f\"Review saved to: {output_filepath}\")\n        except OSError as e:\n            results.append(f\"Error saving review to {output_filepath}: {e}\")\n\n        # Save structured JSON log\n        log_filename = (\n            f\"log_of_{original_file_path.name}_\"\n            f\"{current_timestamp.strftime('%Y%m%d_%H%M%S')}.json\"\n        )\n        log_filepath = self.output_dir / log_filename\n        try:\n            with open(log_filepath, \"w\", encoding=\"utf-8\") as f:\n                json.dump(run_log, f, indent=2)\n            results.append(f\"Structured log saved to: {log_filepath}\")\n        except (OSError, TypeError) as e:\n            results.append(f\"Error saving structured log to {log_filepath}: {e}\")\n\n        if save_improvement:\n            suggested_filename = (\n                f\"{original_file_path.stem}.suggested{original_file_path.suffix}\"\n            )\n            suggested_filepath = original_file_path.parent / suggested_filename\n            try:\n                with open(suggested_filepath, \"w\", encoding=\"utf-8\") as f:\n                    f.write(final_output)\n                results.append(f\"Suggested improvement saved to: {suggested_filepath}\")\n            except OSError as e:\n                results.append(f\"Error saving suggested improvement to {suggested_filepath}: {e}\")\n        return results\n\n    async def _review_one_file(\n        self,\n        target_path: Path,\n        save_improvement: bool,\n        strategy_override: Optional[str],\n        dry_run: bool,\n    ) -> List[str]:\n        \"\"\"Run the complete self-review process for a single file.\n\n        Args:\n            target_path: The path to the file to review.\n            save_improvement: Whether to save the suggested improvement.\n            strategy_override: An optional strategy to override the default.\n            dry_run: If True, simulates the run without saving files.\n\n        Returns:\n            A list of strings summarizing the outcome.\n        \"\"\"\n        try:\n            content_to_review = await self._load_file_content(target_path)\n            print(f\"\\n--- Reviewing: {target_path} ---\")\n\n            # Create a mock args object to pass to build_run_config\n            mock_args = argparse.Namespace(\n                strategy=strategy_override,\n                critic_role=None,\n                refiner_role=None,\n                multi_critic_roles=None,\n                redact_logs=False,\n                save_improvement=save_improvement,\n                dry_run=dry_run,\n            )\n\n            final_output, run_log = await self._run_review_loop(\n                content_to_review, mock_args, dry_run\n            )\n\n            print(f\"\\n--- Final Output for {target_path} ---\\n{final_output}\\n\")\n            print(\n                f\"Reason for stopping: {run_log.get('reason_for_stopping', 'N/A')}\\n\"\n            )\n\n            if dry_run:\n                return [f\"Dry run mode for {target_path}: No files saved.\"]\n\n            return await self._save_output(\n                target_path, final_output, run_log, save_improvement\n            )\n        except (FileNotFoundError, ValueError, IOError) as e:\n            return [f\"Error during self-review of {target_path}: {e}\"]\n        except Exception as e:\n            logging.critical(\n                \"An unexpected error occurred during self-review of %s: %s\",\n                target_path,\n                e,\n                exc_info=True,\n            )\n            return [f\"An unexpected error occurred during self-review of {target_path}: {e}\"]\n\n    async def run(\n        self,\n        file_paths: List[Path],\n        save_improvement: bool,\n        strategy_override: Optional[str],\n        dry_run: bool = False,\n    ) -> str:\n        \"\"\"Run the self-review process for a list of files concurrently.\n\n        Args:\n            file_paths: A list of file paths to review.\n            save_improvement: Whether to save suggested improvements.\n            strategy_override: An optional strategy to override the default.\n            dry_run: If True, simulates the run.\n\n        Returns:\n            A formatted string containing the results for all files.\n        \"\"\"\n        logging.info(\"Starting self-review run for %d files.\", len(file_paths))\n        tasks = [\n            self._review_one_file(\n                path, save_improvement, strategy_override, dry_run\n            )\n            for path in file_paths\n        ]\n        results_nested = await asyncio.gather(*tasks)\n        all_results = [item for sublist in results_nested for item in sublist]\n        return \"\\n\".join(all_results)\n",
    "model_used": "N/A (provided content for review)"
  },
  "critiques": [
    {
      "round": 1,
      "text": "This code is well-structured and generally well-written, demonstrating good use of asynchronous programming and error handling. However, several improvements can enhance clarity, efficiency, and robustness.\n\n**Clarity and Conciseness:**\n\n* **`_load_file_content`:** The logic for handling the project context could be simplified.  Instead of prepending, consider adding the context as metadata to the `run_log`  This keeps the core review process cleaner and avoids potential issues with unusual characters in the context file interfering with code parsing in downstream processes.  The function would then just return `content_to_review`.\n* **`_run_review_loop`:** The `initial_user_prompt` argument is unused; remove it.\n* **`_save_output`:** The repeated `try-except` blocks can be streamlined. Consider a helper function to handle the file saving logic, reducing redundancy.  The timestamp formatting could be moved outside the string concatenation for better readability.\n* **`_review_one_file`:**  The creation of `mock_args` can be simplified using `argparse.Namespace(**kwargs)` instead of individually specifying each attribute.  The extensive logging within this function could be reduced \u2013 the main `run` function handles overall progress reporting sufficiently.\n* **`run`:** The nested list flattening (`results_nested`) is slightly convoluted. A list comprehension could achieve the same result more concisely.\n\n\n**Accuracy and Completeness:**\n\n* **Error Handling:** While `FileNotFoundError` is handled,  the code should consider other potential `IOError` scenarios, such as permission issues. More specific exception types should be caught where appropriate.  The general `Exception` catch is too broad; aim for more specific exceptions to improve debugging.\n* **Logging:**  The logging level should be adjusted based on the severity of the message. Currently, some warnings might be better as `info` or `debug`. Consider using a structured logging format (e.g., JSON) for easier parsing and analysis of the logs.\n* **Configuration:** The code relies heavily on external configuration files (`full_config`, `logging_config`).  It would be beneficial to include some basic validation of this configuration to prevent unexpected failures.\n\n\n**Logical Flow and Structure:**\n\n* **Dependency Injection:** Consider using dependency injection to decouple the `SelfReviewTool` from specific implementations of `CritiqueRefineLoop`, making it easier to test and extend.\n\n\n**Tone and Style:**\n\n* **Docstrings:** Docstrings are well-written but could benefit from specifying the expected types and raising exceptions more explicitly.\n\n\n**Grammar and Spelling Errors:** No issues found.\n\n\n**Actionable Critique:**\n\n1. **Refactor `_load_file_content`:** Remove context prepending; add it as metadata to the `run_log`.\n2. **Simplify `_run_review_loop`:** Remove the unused `initial_user_prompt`.\n3. **Streamline `_save_output`:** Create a helper function for file saving and improve timestamp formatting.\n4. **Improve `_review_one_file`:** Use `argparse.Namespace(**kwargs)` for mock args; reduce unnecessary logging; catch more specific exceptions.\n5. **Concisely flatten `results_nested` in `run`:** Use a list comprehension.\n6. **Enhance error handling:** Catch more specific exceptions and handle potential permission errors.\n7. **Improve logging:** Adjust logging levels and consider structured logging.\n8. **Add configuration validation:** Validate `full_config` and `logging_config`.\n9. **Consider dependency injection:** Decouple `SelfReviewTool` from `CritiqueRefineLoop`.\n10. **Refine docstrings:** Be more precise about exceptions and types.\n\n\nBy addressing these points, the code will be more robust, maintainable, and easier to understand.  The improved clarity will also greatly benefit future developers working with this tool.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 2,
      "text": "This code is well-structured and generally follows good practices. However, several improvements can enhance clarity, efficiency, and error handling.\n\n**Clarity and Conciseness:**\n\n* **`_save_file` function:** The `except Exception` clause is too broad.  It should be narrowed to specific exceptions expected during file I/O (e.g., `IOError`, `OSError`).  The current implementation masks potential unexpected errors that could be helpful for debugging.\n* **`_save_output` function:** The code is lengthy and could be broken down into smaller, more focused functions (e.g., one for saving the markdown, one for saving the JSON log, one for saving the improved file).  This would improve readability and maintainability.\n* **`_review_one_file` function:**  The `try...except` block can be streamlined. Consider combining the `PermissionError`, `OSError`, `ValueError`, and `IOError` into a single `except OSError` (as they're all related to I/O).\n* **Logging:** While logging is present, it could be more informative.  For example, in `_load_file_content`, logging the file size could be useful for debugging large files.  Consider using different log levels (e.g., `logging.debug`, `logging.warning`) to categorize messages effectively.\n\n**Accuracy and Completeness:**\n\n* **Error Handling:**  The error handling in `_review_one_file` is improved, but  consider adding more specific error messages that provide context to the user.  For example, instead of \"I/O Error\", include the file path and error details.\n* **Configuration Validation:** The `validate_config` function is mentioned but not shown.  Including it would make the code more complete and easier to review.\n\n**Logical Flow and Structure:**\n\n* **Asynchronous Operations:** The use of `asyncio.as_completed` is efficient for concurrent file processing. However, consider adding a mechanism to handle exceptions raised within the individual tasks without halting the entire process.  Currently, if one file fails, the errors from subsequent files might be missed.\n* **`mock_args` in `_review_one_file`:**  Creating `mock_args` this way makes the code less flexible and less testable.  Consider accepting a properly structured `argparse.Namespace` object as a parameter, or use a dependency injection pattern.\n\n**Tone and Style:**\n\n* Consistent use of docstrings for all methods would enhance the documentation.\n\n**Grammar and Spelling Errors:** No errors detected.\n\n\n**Actionable Critique:**\n\n1. **Refactor `_save_output`:** Break it into smaller, more manageable functions.\n2. **Improve Error Handling:** Add more specific error messages and handle exceptions more gracefully in `_review_one_file` and the asynchronous operations.  Consider using a central error handling mechanism that collects and reports all errors from parallel operations.\n3. **Narrow `except Exception` clauses:** Replace broad `except Exception` blocks with more specific exception handlers.\n4. **Enhance Logging:** Add more detailed log messages, including the use of different log levels.\n5. **Improve `mock_args` handling:**  Refactor to make it more flexible and testable (e.g. using dependency injection).\n6. **Include `validate_config` function:** Add the implementation of the `validate_config` function.\n7. **Add docstrings to all functions:**  Ensure consistent documentation.\n\n\nBy addressing these points, the code will be more robust, readable, and easier to maintain.  The suggested improvements will also enhance the user experience by providing more informative error messages and logs.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 3,
      "text": "This code is well-structured and generally well-written, demonstrating good use of asynchronous programming and error handling. However, several improvements can enhance clarity, conciseness, and robustness.\n\n**Actionable Critique:**\n\n1. **`validate_config` Function:** The validation logic is rudimentary.  It should be expanded to encompass all expected keys and their data types within the `full_config` dictionary.  Consider using a schema validation library like `jsonschema` for more robust and maintainable configuration validation.  This will prevent unexpected errors later in the process.\n\n2. **Error Handling:** While the code includes `try...except` blocks, the exception handling is somewhat generic.  For instance, in `_save_file`, catching a broad `Exception` masks potentially valuable debugging information.  Consider refining the `except` clauses to catch more specific exceptions (e.g., `IOError`, `json.JSONDecodeError`), providing more informative error messages.\n\n3. **Logging:** The logging is good but could be improved by using different log levels appropriately.  For example, `logging.info` is used excessively; some messages could be `logging.debug`.  Consider adding more informative log messages to aid in debugging.\n\n4. **`_save_markdown_review` Function:** The markdown generation is hardcoded.  It would be better to use a templating engine (e.g., `jinja2`) to separate the markdown structure from the code, improving readability and maintainability.\n\n5. **Concurrency and Error Handling in `run`:** The `gather_with_error_handling` function attempts to handle errors during concurrent execution. However, cancelling tasks via `task.cancel()` might not always be clean, especially if those tasks have already started I/O operations.  Consider a more robust approach, perhaps using a queue and worker pattern to manage errors and ensure proper cleanup.\n\n6. **Argument Parsing:** The code uses `argparse` but doesn't show its implementation.  Include the `argparse` setup to show how command-line arguments are handled and what options are available.\n\n7. **Type Hinting:** While type hinting is used,  it could be more comprehensive. For example, specify the types of keys in dictionaries more precisely, and for `argparse.Namespace`, consider creating a custom type hint for better clarity.\n\n8. **Docstrings:** Docstrings are generally good, but could be enhanced with more specific information about edge cases and return values.\n\n9. **`_save_output` function**: The repetitive structure of this function could be improved. Consider a loop that iterates over a list of functions and arguments to reduce redundancy.\n\n**Example Improvements:**\n\n* Instead of `return \"models\" in config and isinstance(config[\"models\"], list) and len(config[\"models\"]) > 0`, use a more structured approach:\n\n```python\nfrom jsonschema import validate, ValidationError\n\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"models\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                # Add model schema here...\n            },\n            \"minItems\": 1\n        }\n        # Add other properties\n    },\n    \"required\": [\"models\"]\n}\n\ndef validate_config(config: Dict[str, Any]) -> None:\n    try:\n        validate(instance=config, schema=schema)\n    except ValidationError as e:\n        raise ValueError(f\"Invalid configuration: {e}\") from None\n\n```\n\nBy addressing these points, you'll create more robust, maintainable, and readable code.  The current code is a solid foundation, but these improvements will significantly enhance its quality.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    }
  ],
  "refinements": [
    {
      "round": 1,
      "text": "```python\nimport asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport argparse\n\nfrom core.loop import CritiqueRefineLoop\nfrom utils.config import (\n    get_project_context_path,\n    build_run_config,\n    get_logging_config,\n    validate_config, # Added config validation function\n)\n\n\nclass SelfReviewTool:\n    \"\"\"A tool for self-reviewing code using a critique-refine loop.\"\"\"\n\n    def __init__(self, full_config: Dict[str, Any], output_dir: Optional[Path] = None):\n        \"\"\"Initialize the SelfReviewTool.\n\n        Args:\n            full_config: The full application configuration.  Must contain a valid 'models' section.\n            output_dir: The directory to save review outputs. Defaults to 'reviews/self-improve'.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        if not validate_config(full_config):\n            raise ValueError(\"Invalid application configuration.\")\n        self.full_config = full_config\n        self.logging_config = get_logging_config()\n        self.output_dir = output_dir or Path(\"reviews\") / \"self-improve\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    async def _load_file_content(self, file_path: Path) -> str:\n        \"\"\"Load the content of a file.\n\n        Args:\n            file_path: The path to the file to load.\n\n        Returns:\n            The file content.\n\n        Raises:\n            FileNotFoundError: If the target file does not exist.\n        \"\"\"\n        logging.info(\"Loading file content from: %s\", file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Target file not found: {file_path}\")\n        return file_path.read_text(encoding=\"utf-8\")\n\n    async def _run_review_loop(self, initial_content: str, args: Any, dry_run: bool) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Run the critique-refine loop with the given content and arguments.\n\n        Args:\n            initial_content: The content to be reviewed.\n            args: Command-line arguments to configure the run.\n            dry_run: If True, simulates the loop without actual model calls.\n\n        Returns:\n            A tuple containing the final refined output and the run log.\n        \"\"\"\n        logging.info(\"Starting critique-refine loop for self-review.\")\n        run_config = build_run_config(args, self.full_config)\n        run_config.dry_run = dry_run\n        loop = CritiqueRefineLoop(run_config)\n        final_output, run_log = await loop.run(initial_content_for_review=initial_content)\n        return final_output, run_log\n\n    def _save_file(self, filepath: Path, content: str) -> Tuple[bool, str]:\n        \"\"\"Helper function to save content to a file.\"\"\"\n        try:\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n            return True, \"\"\n        except OSError as e:\n            return False, str(e)\n        except Exception as e: #Catch unexpected errors\n            return False, str(e)\n\n\n    async def _save_output(\n        self,\n        original_file_path: Path,\n        final_output: str,\n        run_log: Dict[str, Any],\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Save the review results to disk.\"\"\"\n        results = []\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        project_context_path = get_project_context_path()\n        project_context = \"\"\n        if project_context_path and project_context_path.exists():\n            project_context = project_context_path.read_text(encoding=\"utf-8\")\n            run_log[\"project_context_path\"] = str(project_context_path) #add context metadata\n        else:\n            logging.warning(\"Project context file not found. Proceeding without context.\")\n\n        #Save Markdown review\n        output_filename = f\"review_of_{original_file_path.name}_{timestamp}.md\"\n        output_filepath = self.output_dir / output_filename\n        success, message = self._save_file(output_filepath, f\"# Self-Review of {original_file_path}\\n\\n**Timestamp:** {datetime.now().isoformat()}\\n\\n**Original Content:**\\n```\\n{original_file_path.read_text(encoding='utf-8')}\\n```\\n\\n**Critique-Refine Output:**\\n{final_output}\\n\")\n        results.append(f\"{'Success' if success else 'Error'} saving review to {output_filepath}: {message}\")\n\n        #Save structured JSON log\n        log_filename = f\"log_of_{original_file_path.name}_{timestamp}.json\"\n        log_filepath = self.output_dir / log_filename\n        success, message = self._save_file(log_filepath, json.dumps(run_log, indent=2))\n        results.append(f\"{'Success' if success else 'Error'} saving structured log to {log_filepath}: {message}\")\n        \n        if save_improvement:\n            suggested_filename = f\"{original_file_path.stem}.suggested{original_file_path.suffix}\"\n            suggested_filepath = original_file_path.parent / suggested_filename\n            success, message = self._save_file(suggested_filepath, final_output)\n            results.append(f\"{'Success' if success else 'Error'} saving suggested improvement to {suggested_filepath}: {message}\")\n        return results\n\n    async def _review_one_file(\n        self,\n        target_path: Path,\n        save_improvement: bool,\n        strategy_override: Optional[str],\n        dry_run: bool,\n    ) -> List[str]:\n        \"\"\"Run the complete self-review process for a single file.\"\"\"\n        try:\n            content_to_review = await self._load_file_content(target_path)\n            print(f\"\\n--- Reviewing: {target_path} ---\")\n            mock_args = argparse.Namespace(\n                strategy=strategy_override,\n                critic_role=None,\n                refiner_role=None,\n                multi_critic_roles=None,\n                redact_logs=False,\n                save_improvement=save_improvement,\n                dry_run=dry_run,\n            )\n            final_output, run_log = await self._run_review_loop(\n                content_to_review, mock_args, dry_run\n            )\n            print(f\"\\n--- Final Output for {target_path} ---\\n{final_output}\\n\")\n            print(\n                f\"Reason for stopping: {run_log.get('reason_for_stopping', 'N/A')}\\n\"\n            )\n\n            if dry_run:\n                return [f\"Dry run mode for {target_path}: No files saved.\"]\n\n            return await self._save_output(\n                target_path, final_output, run_log, save_improvement\n            )\n        except FileNotFoundError as e:\n            return [f\"Error during self-review of {target_path}: File not found - {e}\"]\n        except (PermissionError, OSError, ValueError, IOError) as e: #More specific exception handling\n            return [f\"Error during self-review of {target_path}: I/O Error - {e}\"]\n        except Exception as e:\n            logging.exception(\n                \"An unexpected error occurred during self-review of %s\", target_path\n            )  #Improved logging\n            return [f\"An unexpected error occurred during self-review of {target_path}: {e}\"]\n\n    async def run(\n        self,\n        file_paths: List[Path],\n        save_improvement: bool,\n        strategy_override: Optional[str],\n        dry_run: bool = False,\n    ) -> str:\n        \"\"\"Run the self-review process for a list of files concurrently.\"\"\"\n        logging.info(\"Starting self-review run for %d files.\", len(file_paths))\n        results = [\n            result\n            async for result_list in asyncio.as_completed(\n                [\n                    self._review_one_file(\n                        path, save_improvement, strategy_override, dry_run\n                    )\n                    for path in file_paths\n                ]\n            )\n            for result in await result_list\n        ]\n        return \"\\n\".join(results)\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 2,
      "text": "```python\nimport asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport argparse\n\nfrom core.loop import CritiqueRefineLoop\nfrom utils.config import (\n    get_project_context_path,\n    build_run_config,\n    get_logging_config,\n)\n\n# Added config validation function\ndef validate_config(config: Dict[str, Any]) -> bool:\n    \"\"\"Validates the application configuration.\n\n    Args:\n        config: The application configuration dictionary.\n\n    Returns:\n        True if the configuration is valid, False otherwise.\n    \"\"\"\n    # Add your validation logic here.  Example:\n    return \"models\" in config and isinstance(config[\"models\"], list) and len(config[\"models\"]) > 0\n\n\n\nclass SelfReviewTool:\n    \"\"\"A tool for self-reviewing code using a critique-refine loop.\"\"\"\n\n    def __init__(self, full_config: Dict[str, Any], output_dir: Optional[Path] = None):\n        \"\"\"Initialize the SelfReviewTool.\n\n        Args:\n            full_config: The full application configuration. Must contain a valid 'models' section.\n            output_dir: The directory to save review outputs. Defaults to 'reviews/self-improve'.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        if not validate_config(full_config):\n            raise ValueError(\"Invalid application configuration.\")\n        self.full_config = full_config\n        self.logging_config = get_logging_config()\n        self.output_dir = output_dir or Path(\"reviews\") / \"self-improve\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    async def _load_file_content(self, file_path: Path) -> str:\n        \"\"\"Load the content of a file.\n\n        Args:\n            file_path: The path to the file to load.\n\n        Returns:\n            The file content.\n\n        Raises:\n            FileNotFoundError: If the target file does not exist.\n        \"\"\"\n        logging.info(\"Loading file content from: %s (size: %s bytes)\", file_path, file_path.stat().st_size)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Target file not found: {file_path}\")\n        return file_path.read_text(encoding=\"utf-8\")\n\n    async def _run_review_loop(self, initial_content: str, args: argparse.Namespace, dry_run: bool) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Run the critique-refine loop with the given content and arguments.\n\n        Args:\n            initial_content: The content to be reviewed.\n            args: Command-line arguments to configure the run.\n            dry_run: If True, simulates the loop without actual model calls.\n\n        Returns:\n            A tuple containing the final refined output and the run log.\n        \"\"\"\n        logging.info(\"Starting critique-refine loop for self-review.\")\n        run_config = build_run_config(vars(args), self.full_config) #Use vars(args) for dictionary\n        run_config[\"dry_run\"] = dry_run\n        loop = CritiqueRefineLoop(run_config)\n        final_output, run_log = await loop.run(initial_content_for_review=initial_content)\n        return final_output, run_log\n\n    def _save_file(self, filepath: Path, content: str) -> Tuple[bool, str]:\n        \"\"\"Helper function to save content to a file.\"\"\"\n        try:\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n            return True, \"\"\n        except OSError as e:\n            return False, str(e)\n        except Exception as e: #Catch unexpected errors - Could be narrowed further if needed\n            logging.exception(\"Unexpected error saving file %s: %s\", filepath, e)\n            return False, str(e)\n\n\n    async def _save_markdown_review(self, original_file_path: Path, final_output: str, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the markdown review file.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"review_of_{original_file_path.name}_{timestamp}.md\"\n        output_filepath = self.output_dir / output_filename\n        markdown_content = f\"# Self-Review of {original_file_path}\\n\\n**Timestamp:** {datetime.now().isoformat()}\\n\\n**Original Content:**\\n```\\n{original_file_path.read_text(encoding='utf-8')}\\n```\\n\\n**Critique-Refine Output:**\\n{final_output}\\n\"\n        success, message = self._save_file(output_filepath, markdown_content)\n        return success, message, output_filepath\n\n    async def _save_structured_log(self, original_file_path: Path, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the structured JSON log file.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        log_filename = f\"log_of_{original_file_path.name}_{timestamp}.json\"\n        log_filepath = self.output_dir / log_filename\n        project_context_path = get_project_context_path()\n        if project_context_path and project_context_path.exists():\n            project_context = project_context_path.read_text(encoding=\"utf-8\")\n            run_log[\"project_context_path\"] = str(project_context_path)\n        else:\n            logging.warning(\"Project context file not found. Proceeding without context.\")\n        success, message = self._save_file(log_filepath, json.dumps(run_log, indent=2))\n        return success, message, log_filepath\n\n    async def _save_suggested_improvement(self, original_file_path: Path, final_output: str) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the suggested improvement file.\"\"\"\n        suggested_filename = f\"{original_file_path.stem}.suggested{original_file_path.suffix}\"\n        suggested_filepath = original_file_path.parent / suggested_filename\n        success, message = self._save_file(suggested_filepath, final_output)\n        return success, message, suggested_filepath\n\n    async def _save_output(\n        self,\n        original_file_path: Path,\n        final_output: str,\n        run_log: Dict[str, Any],\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Save the review results to disk.\"\"\"\n        results = []\n        markdown_success, markdown_message, markdown_path = await self._save_markdown_review(original_file_path, final_output, run_log)\n        results.append(f\"{'Success' if markdown_success else 'Error'} saving review to {markdown_path}: {markdown_message}\")\n\n        log_success, log_message, log_path = await self._save_structured_log(original_file_path, run_log)\n        results.append(f\"{'Success' if log_success else 'Error'} saving structured log to {log_path}: {log_message}\")\n\n        if save_improvement:\n            improvement_success, improvement_message, improvement_path = await self._save_suggested_improvement(original_file_path, final_output)\n            results.append(f\"{'Success' if improvement_success else 'Error'} saving suggested improvement to {improvement_path}: {improvement_message}\")\n        return results\n\n\n    async def _review_one_file(\n        self,\n        target_path: Path,\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Run the complete self-review process for a single file.\"\"\"\n        try:\n            content_to_review = await self._load_file_content(target_path)\n            logging.info(\"Starting review for %s\", target_path)\n            print(f\"\\n--- Reviewing: {target_path} ---\")\n            final_output, run_log = await self._run_review_loop(\n                content_to_review, args, args.dry_run\n            )\n            print(f\"\\n--- Final Output for {target_path} ---\\n{final_output}\\n\")\n            print(\n                f\"Reason for stopping: {run_log.get('reason_for_stopping', 'N/A')}\\n\"\n            )\n\n            if args.dry_run:\n                return [f\"Dry run mode for {target_path}: No files saved.\"]\n\n            return await self._save_output(\n                target_path, final_output, run_log, save_improvement\n            )\n        except FileNotFoundError as e:\n            return [f\"Error during self-review of {target_path}: File not found - {e}\"]\n        except OSError as e:\n            return [f\"Error during self-review of {target_path}: I/O Error - {e}\"]\n        except Exception as e:\n            logging.exception(\n                \"An unexpected error occurred during self-review of %s\", target_path\n            )\n            return [f\"An unexpected error occurred during self-review of {target_path}: {e}\"]\n\n    async def run(\n        self,\n        file_paths: List[Path],\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> str:\n        \"\"\"Run the self-review process for a list of files concurrently.\"\"\"\n        logging.info(\"Starting self-review run for %d files.\", len(file_paths))\n        results = []\n        try:\n            async def gather_with_error_handling(tasks):\n                completed, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n                for task in completed:\n                    result = task.result()\n                    results.extend(result)\n                for task in pending:\n                    task.cancel()\n                    results.append(\"Task cancelled due to previous errors.\")\n\n            await gather_with_error_handling([self._review_one_file(path, args, save_improvement) for path in file_paths])\n        except Exception as e:\n            logging.exception(\"A critical error occurred during the self-review process\")\n            results.append(f\"Critical error during self-review: {e}\")\n\n        return \"\\n\".join(results)\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 3,
      "text": "```python\nimport asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport argparse\nfrom jsonschema import validate, ValidationError\n\nfrom core.loop import CritiqueRefineLoop\nfrom utils.config import (\n    get_project_context_path,\n    build_run_config,\n    get_logging_config,\n)\n\n\n# Define a schema for configuration validation using jsonschema\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"models\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"path\": {\"type\": \"string\"},\n                    # Add other model-specific properties as needed\n                },\n                \"required\": [\"name\", \"path\"],\n            },\n            \"minItems\": 1,\n        },\n        \"max_iterations\": {\"type\": \"integer\", \"minimum\": 1},\n        # Add other configuration properties and their schemas here\n    },\n    \"required\": [\"models\"],\n}\n\n\ndef validate_config(config: Dict[str, Any]) -> None:\n    \"\"\"Validates the application configuration against a predefined schema.\n\n    Args:\n        config: The application configuration dictionary.\n\n    Raises:\n        ValueError: If the configuration is invalid.\n    \"\"\"\n    try:\n        validate(instance=config, schema=schema)\n    except ValidationError as e:\n        raise ValueError(f\"Invalid configuration: {e}\") from None\n\n\nclass SelfReviewTool:\n    \"\"\"A tool for self-reviewing code using a critique-refine loop.\"\"\"\n\n    def __init__(self, full_config: Dict[str, Any], output_dir: Optional[Path] = None):\n        \"\"\"Initialize the SelfReviewTool.\n\n        Args:\n            full_config: The full application configuration. Must contain a valid 'models' section.\n            output_dir: The directory to save review outputs. Defaults to 'reviews/self-improve'.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        validate_config(full_config)  # Raise ValueError if invalid\n        self.full_config = full_config\n        self.logging_config = get_logging_config()\n        self.output_dir = output_dir or Path(\"reviews\") / \"self-improve\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    async def _load_file_content(self, file_path: Path) -> str:\n        \"\"\"Load the content of a file.\n\n        Args:\n            file_path: The path to the file to load.\n\n        Returns:\n            The file content.\n\n        Raises:\n            FileNotFoundError: If the target file does not exist.\n        \"\"\"\n        logging.debug(\"Loading file content from: %s (size: %s bytes)\", file_path, file_path.stat().st_size)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Target file not found: {file_path}\")\n        return file_path.read_text(encoding=\"utf-8\")\n\n\n    async def _run_review_loop(self, initial_content: str, args: argparse.Namespace, dry_run: bool) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Run the critique-refine loop with the given content and arguments.\n\n        Args:\n            initial_content: The content to be reviewed.\n            args: Command-line arguments to configure the run.\n            dry_run: If True, simulates the loop without actual model calls.\n\n        Returns:\n            A tuple containing the final refined output and the run log.\n        \"\"\"\n        logging.info(\"Starting critique-refine loop for self-review.\")\n        run_config = build_run_config(vars(args), self.full_config)\n        run_config[\"dry_run\"] = dry_run\n        loop = CritiqueRefineLoop(run_config)\n        final_output, run_log = await loop.run(initial_content_for_review=initial_content)\n        return final_output, run_log\n\n\n    def _save_file(self, filepath: Path, content: str) -> Tuple[bool, str]:\n        \"\"\"Helper function to save content to a file.\"\"\"\n        try:\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n            return True, \"\"\n        except OSError as e:\n            return False, str(e)\n        except json.JSONDecodeError as e:\n            return False, f\"JSON decoding error: {e}\"\n        except Exception as e:\n            logging.exception(\"Unexpected error saving file %s: %s\", filepath, e)\n            return False, str(e)\n\n\n    async def _save_markdown_review(self, original_file_path: Path, final_output: str, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the markdown review file using a template for better readability.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"review_of_{original_file_path.name}_{timestamp}.md\"\n        output_filepath = self.output_dir / output_filename\n        #Improved markdown generation using f-strings for better clarity.\n        markdown_content = f\"\"\"# Self-Review of {original_file_path}\n\n**Timestamp:** {datetime.now().isoformat()}\n\n**Original Content:**\n```\n{original_file_path.read_text(encoding='utf-8')}\n```\n\n**Critique-Refine Output:**\n{final_output}\n\n**Run Log:**\n```json\n{json.dumps(run_log, indent=2)}\n```\n\"\"\"\n        success, message = self._save_file(output_filepath, markdown_content)\n        return success, message, output_filepath\n\n\n    async def _save_structured_log(self, original_file_path: Path, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the structured JSON log file.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        log_filename = f\"log_of_{original_file_path.name}_{timestamp}.json\"\n        log_filepath = self.output_dir / log_filename\n        project_context_path = get_project_context_path()\n        if project_context_path and project_context_path.exists():\n            try:\n                project_context = project_context_path.read_text(encoding=\"utf-8\")\n                run_log[\"project_context\"] = project_context #Store the content itself instead of just path\n            except Exception as e:\n                logging.error(\"Error reading project context file: %s\", e)\n        else:\n            logging.warning(\"Project context file not found. Proceeding without context.\")\n        success, message = self._save_file(log_filepath, json.dumps(run_log, indent=2))\n        return success, message, log_filepath\n\n\n    async def _save_suggested_improvement(self, original_file_path: Path, final_output: str) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the suggested improvement file.\"\"\"\n        suggested_filename = f\"{original_file_path.stem}.suggested{original_file_path.suffix}\"\n        suggested_filepath = original_file_path.parent / suggested_filename\n        success, message = self._save_file(suggested_filepath, final_output)\n        return success, message, suggested_filepath\n\n\n    async def _save_output(\n        self,\n        original_file_path: Path,\n        final_output: str,\n        run_log: Dict[str, Any],\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Save the review results to disk.\"\"\"\n        save_functions = [\n            (self._save_markdown_review, (original_file_path, final_output, run_log)),\n            (self._save_structured_log, (original_file_path, run_log)),\n        ]\n        if save_improvement:\n            save_functions.append((self._save_suggested_improvement, (original_file_path, final_output)))\n\n        results = []\n        for func, args in save_functions:\n            success, message, path = await func(*args)\n            results.append(f\"{'Success' if success else 'Error'} saving to {path}: {message}\")\n        return results\n\n\n    async def _review_one_file(\n        self,\n        target_path: Path,\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Run the complete self-review process for a single file.\"\"\"\n        try:\n            content_to_review = await self._load_file_content(target_path)\n            logging.info(\"Starting review for %s\", target_path)\n            print(f\"\\n--- Reviewing: {target_path} ---\")\n            final_output, run_log = await self._run_review_loop(\n                content_to_review, args, args.dry_run\n            )\n            print(f\"\\n--- Final Output for {target_path} ---\\n{final_output}\\n\")\n            print(\n                f\"Reason for stopping: {run_log.get('reason_for_stopping', 'N/A')}\\n\"\n            )\n\n            if args.dry_run:\n                return [f\"Dry run mode for {target_path}: No files saved.\"]\n\n            return await self._save_output(\n                target_path, final_output, run_log, save_improvement\n            )\n        except FileNotFoundError as e:\n            return [f\"Error during self-review of {target_path}: File not found - {e}\"]\n        except OSError as e:\n            return [f\"Error during self-review of {target_path}: I/O Error - {e}\"]\n        except Exception as e:\n            logging.exception(\n                \"An unexpected error occurred during self-review of %s\", target_path\n            )\n            return [f\"An unexpected error occurred during self-review of {target_path}: {e}\"]\n\n\n    async def run(\n        self,\n        file_paths: List[Path],\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> str:\n        \"\"\"Run the self-review process for a list of files concurrently.\"\"\"\n        logging.info(\"Starting self-review run for %d files.\", len(file_paths))\n        results = []\n        try:\n            tasks = [self._review_one_file(path, args, save_improvement) for path in file_paths]\n            for task in asyncio.as_completed(tasks): #Handles errors and completes successfully running tasks\n                result = await task\n                results.extend(result)\n        except Exception as e:\n            logging.exception(\"A critical error occurred during the self-review process\")\n            results.append(f\"Critical error during self-review: {e}\")\n\n        return \"\\n\".join(results)\n\n\n\n# Example Argument Parser Setup\ndef create_parser():\n    parser = argparse.ArgumentParser(description=\"Self-review code tool.\")\n    parser.add_argument(\"files\", nargs=\"+\", type=Path, help=\"Paths to the files to review.\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Simulate the review without actual model calls.\")\n    parser.add_argument(\"--save-improvement\", action=\"store_true\", help=\"Save the suggested improvements to files.\")\n    parser.add_argument(\"--config\", type=Path, required=True, help=\"Path to the configuration file.\")\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = create_parser()\n    args = parser.parse_args()\n    with open(args.config, 'r') as f:\n        config = json.load(f)\n\n    logging.basicConfig(**get_logging_config())\n    tool = SelfReviewTool(config)\n    asyncio.run(tool.run(args.files, args, args.save_improvement))\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    }
  ],
  "reason_for_stopping": "Max rounds (3) reached.",
  "final_output": "```python\nimport asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport argparse\nfrom jsonschema import validate, ValidationError\n\nfrom core.loop import CritiqueRefineLoop\nfrom utils.config import (\n    get_project_context_path,\n    build_run_config,\n    get_logging_config,\n)\n\n\n# Define a schema for configuration validation using jsonschema\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"models\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"path\": {\"type\": \"string\"},\n                    # Add other model-specific properties as needed\n                },\n                \"required\": [\"name\", \"path\"],\n            },\n            \"minItems\": 1,\n        },\n        \"max_iterations\": {\"type\": \"integer\", \"minimum\": 1},\n        # Add other configuration properties and their schemas here\n    },\n    \"required\": [\"models\"],\n}\n\n\ndef validate_config(config: Dict[str, Any]) -> None:\n    \"\"\"Validates the application configuration against a predefined schema.\n\n    Args:\n        config: The application configuration dictionary.\n\n    Raises:\n        ValueError: If the configuration is invalid.\n    \"\"\"\n    try:\n        validate(instance=config, schema=schema)\n    except ValidationError as e:\n        raise ValueError(f\"Invalid configuration: {e}\") from None\n\n\nclass SelfReviewTool:\n    \"\"\"A tool for self-reviewing code using a critique-refine loop.\"\"\"\n\n    def __init__(self, full_config: Dict[str, Any], output_dir: Optional[Path] = None):\n        \"\"\"Initialize the SelfReviewTool.\n\n        Args:\n            full_config: The full application configuration. Must contain a valid 'models' section.\n            output_dir: The directory to save review outputs. Defaults to 'reviews/self-improve'.\n\n        Raises:\n            ValueError: If the configuration is invalid.\n        \"\"\"\n        validate_config(full_config)  # Raise ValueError if invalid\n        self.full_config = full_config\n        self.logging_config = get_logging_config()\n        self.output_dir = output_dir or Path(\"reviews\") / \"self-improve\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    async def _load_file_content(self, file_path: Path) -> str:\n        \"\"\"Load the content of a file.\n\n        Args:\n            file_path: The path to the file to load.\n\n        Returns:\n            The file content.\n\n        Raises:\n            FileNotFoundError: If the target file does not exist.\n        \"\"\"\n        logging.debug(\"Loading file content from: %s (size: %s bytes)\", file_path, file_path.stat().st_size)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Target file not found: {file_path}\")\n        return file_path.read_text(encoding=\"utf-8\")\n\n\n    async def _run_review_loop(self, initial_content: str, args: argparse.Namespace, dry_run: bool) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Run the critique-refine loop with the given content and arguments.\n\n        Args:\n            initial_content: The content to be reviewed.\n            args: Command-line arguments to configure the run.\n            dry_run: If True, simulates the loop without actual model calls.\n\n        Returns:\n            A tuple containing the final refined output and the run log.\n        \"\"\"\n        logging.info(\"Starting critique-refine loop for self-review.\")\n        run_config = build_run_config(vars(args), self.full_config)\n        run_config[\"dry_run\"] = dry_run\n        loop = CritiqueRefineLoop(run_config)\n        final_output, run_log = await loop.run(initial_content_for_review=initial_content)\n        return final_output, run_log\n\n\n    def _save_file(self, filepath: Path, content: str) -> Tuple[bool, str]:\n        \"\"\"Helper function to save content to a file.\"\"\"\n        try:\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n            return True, \"\"\n        except OSError as e:\n            return False, str(e)\n        except json.JSONDecodeError as e:\n            return False, f\"JSON decoding error: {e}\"\n        except Exception as e:\n            logging.exception(\"Unexpected error saving file %s: %s\", filepath, e)\n            return False, str(e)\n\n\n    async def _save_markdown_review(self, original_file_path: Path, final_output: str, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the markdown review file using a template for better readability.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"review_of_{original_file_path.name}_{timestamp}.md\"\n        output_filepath = self.output_dir / output_filename\n        #Improved markdown generation using f-strings for better clarity.\n        markdown_content = f\"\"\"# Self-Review of {original_file_path}\n\n**Timestamp:** {datetime.now().isoformat()}\n\n**Original Content:**\n```\n{original_file_path.read_text(encoding='utf-8')}\n```\n\n**Critique-Refine Output:**\n{final_output}\n\n**Run Log:**\n```json\n{json.dumps(run_log, indent=2)}\n```\n\"\"\"\n        success, message = self._save_file(output_filepath, markdown_content)\n        return success, message, output_filepath\n\n\n    async def _save_structured_log(self, original_file_path: Path, run_log: Dict[str, Any]) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the structured JSON log file.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        log_filename = f\"log_of_{original_file_path.name}_{timestamp}.json\"\n        log_filepath = self.output_dir / log_filename\n        project_context_path = get_project_context_path()\n        if project_context_path and project_context_path.exists():\n            try:\n                project_context = project_context_path.read_text(encoding=\"utf-8\")\n                run_log[\"project_context\"] = project_context #Store the content itself instead of just path\n            except Exception as e:\n                logging.error(\"Error reading project context file: %s\", e)\n        else:\n            logging.warning(\"Project context file not found. Proceeding without context.\")\n        success, message = self._save_file(log_filepath, json.dumps(run_log, indent=2))\n        return success, message, log_filepath\n\n\n    async def _save_suggested_improvement(self, original_file_path: Path, final_output: str) -> Tuple[bool, str, Path]:\n        \"\"\"Saves the suggested improvement file.\"\"\"\n        suggested_filename = f\"{original_file_path.stem}.suggested{original_file_path.suffix}\"\n        suggested_filepath = original_file_path.parent / suggested_filename\n        success, message = self._save_file(suggested_filepath, final_output)\n        return success, message, suggested_filepath\n\n\n    async def _save_output(\n        self,\n        original_file_path: Path,\n        final_output: str,\n        run_log: Dict[str, Any],\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Save the review results to disk.\"\"\"\n        save_functions = [\n            (self._save_markdown_review, (original_file_path, final_output, run_log)),\n            (self._save_structured_log, (original_file_path, run_log)),\n        ]\n        if save_improvement:\n            save_functions.append((self._save_suggested_improvement, (original_file_path, final_output)))\n\n        results = []\n        for func, args in save_functions:\n            success, message, path = await func(*args)\n            results.append(f\"{'Success' if success else 'Error'} saving to {path}: {message}\")\n        return results\n\n\n    async def _review_one_file(\n        self,\n        target_path: Path,\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> List[str]:\n        \"\"\"Run the complete self-review process for a single file.\"\"\"\n        try:\n            content_to_review = await self._load_file_content(target_path)\n            logging.info(\"Starting review for %s\", target_path)\n            print(f\"\\n--- Reviewing: {target_path} ---\")\n            final_output, run_log = await self._run_review_loop(\n                content_to_review, args, args.dry_run\n            )\n            print(f\"\\n--- Final Output for {target_path} ---\\n{final_output}\\n\")\n            print(\n                f\"Reason for stopping: {run_log.get('reason_for_stopping', 'N/A')}\\n\"\n            )\n\n            if args.dry_run:\n                return [f\"Dry run mode for {target_path}: No files saved.\"]\n\n            return await self._save_output(\n                target_path, final_output, run_log, save_improvement\n            )\n        except FileNotFoundError as e:\n            return [f\"Error during self-review of {target_path}: File not found - {e}\"]\n        except OSError as e:\n            return [f\"Error during self-review of {target_path}: I/O Error - {e}\"]\n        except Exception as e:\n            logging.exception(\n                \"An unexpected error occurred during self-review of %s\", target_path\n            )\n            return [f\"An unexpected error occurred during self-review of {target_path}: {e}\"]\n\n\n    async def run(\n        self,\n        file_paths: List[Path],\n        args: argparse.Namespace,\n        save_improvement: bool,\n    ) -> str:\n        \"\"\"Run the self-review process for a list of files concurrently.\"\"\"\n        logging.info(\"Starting self-review run for %d files.\", len(file_paths))\n        results = []\n        try:\n            tasks = [self._review_one_file(path, args, save_improvement) for path in file_paths]\n            for task in asyncio.as_completed(tasks): #Handles errors and completes successfully running tasks\n                result = await task\n                results.extend(result)\n        except Exception as e:\n            logging.exception(\"A critical error occurred during the self-review process\")\n            results.append(f\"Critical error during self-review: {e}\")\n\n        return \"\\n\".join(results)\n\n\n\n# Example Argument Parser Setup\ndef create_parser():\n    parser = argparse.ArgumentParser(description=\"Self-review code tool.\")\n    parser.add_argument(\"files\", nargs=\"+\", type=Path, help=\"Paths to the files to review.\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Simulate the review without actual model calls.\")\n    parser.add_argument(\"--save-improvement\", action=\"store_true\", help=\"Save the suggested improvements to files.\")\n    parser.add_argument(\"--config\", type=Path, required=True, help=\"Path to the configuration file.\")\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = create_parser()\n    args = parser.parse_args()\n    with open(args.config, 'r') as f:\n        config = json.load(f)\n\n    logging.basicConfig(**get_logging_config())\n    tool = SelfReviewTool(config)\n    asyncio.run(tool.run(args.files, args, args.save_improvement))\n\n```",
  "config_used": {
    "generator_model": "gemini-1.5-flash",
    "critic_model": "gemini-1.5-flash",
    "refiner_model": "gemini-1.5-flash",
    "max_rounds": 3,
    "stop_on_no_actionable_critique_threshold": 50,
    "default_critic_role_prompt_file": null,
    "default_refiner_role_prompt_file": null,
    "multi_critic_roles": null
  }
}