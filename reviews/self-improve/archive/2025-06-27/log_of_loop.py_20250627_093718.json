{
  "timestamp": "2025-06-27T09:35:59.117837",
  "original_user_prompt": "",
  "context_documents": [],
  "initial_generation": {
    "text": "\n\n---\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\n# Local imports\nfrom core.model_router import call_model\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"\n    Encapsulates the logic for the critique and refinement loop.\n    \"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"\n        Initializes the loop with a specific run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.run_log: Dict[str, Any] = {}\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Handle the initial content generation or loading phase.\n\n        If initial content is provided, it's used directly. Otherwise, the generator\n        model is called with the user's prompt.\n\n        Args:\n            initial_user_prompt: The user's starting prompt.\n            initial_content_for_review: Existing content to be reviewed.\n\n        Returns:\n            A tuple containing the initial text and a log dictionary.\n        \"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_generation_log = {}\n        if initial_content_for_review:\n            initial_response = initial_content_for_review\n            initial_generation_log = {\n                \"text\": initial_response,\n                \"model_used\": \"N/A (provided content for review)\",\n            }\n            logging.info(\"\\n--- Content for Review ---\\n%s\\n\", initial_response)\n        else:\n            initial_response = await call_model(\n                prompt=f\"User prompt: {initial_user_prompt}\",\n                model_name=self.run_config.generator_model,\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"generator\",\n            )\n            initial_generation_log = {\n                \"text\": initial_response,\n                \"model_used\": self.run_config.generator_model,\n            }\n            logging.info(\"\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generate a critique for the given text.\n\n        If multiple critic roles are configured, it runs them in parallel and\n        combines their critiques. Otherwise, it uses a single critic.\n\n        Args:\n            current_text: The text to be critiqued.\n            round_num: The current iteration number.\n\n        Returns:\n            A tuple containing the critique and a log dictionary.\n\n        Raises:\n            ValueError: If a required prompt template is not found.\n        \"\"\"\n        logging.info(\"Entering critique phase (Round %d)...\", round_num)\n        critique_log = {}\n        if self.run_config.multi_critic_roles:\n            logging.info(\n                \"--- Multi-agent Critique (Roles: %s) ---\",\n                \", \".join(self.run_config.multi_critic_roles),\n            )\n\n            async def get_critique(role_file):\n                role_template = load_role_template(role_file)\n                return await call_model(\n                    prompt=current_text,\n                    model_name=self.run_config.critic_model,\n                    system_prompt=role_template,\n                    config=self.run_config.full_config,\n                    dry_run=self.run_config.dry_run,\n                    role=\"critic\",\n                )\n\n            try:\n                critiques = await asyncio.gather(\n                    *[\n                        get_critique(role_file)\n                        for role_file in self.run_config.multi_critic_roles\n                    ]\n                )\n            except (TemplateNotFoundError, IOError) as e:\n                logging.error(\"Failed to load critic prompt template: %s\", e)\n                raise ValueError(\n                    \"Failed to load one or more critic prompt templates.\"\n                ) from e\n            critique = \"\\n\\n\".join(critiques)\n            critique_log = {\n                \"round\": round_num,\n                \"text\": critique,\n                \"model_used\": self.run_config.critic_model,\n                \"role_prompt_file_used\": self.run_config.multi_critic_roles,\n            }\n        else:\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.run_config.critic_model,\n                system_prompt=self.run_config.roles.get(\"critic_template\"),\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"critic\",\n            )\n            critique_log = {\n                \"round\": round_num,\n                \"text\": critique,\n                \"model_used\": self.run_config.critic_model,\n                \"role_prompt_file_used\": self.run_config.default_critic_role_prompt_file,\n            }\n        logging.info(\"\\n--- Critique ---\\n%s\\n\", critique)\n        return critique, critique_log\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refine the text based on the provided critique.\n\n        Args:\n            current_text: The text to be refined.\n            critique: The critique to apply.\n            round_num: The current iteration number.\n\n        Returns:\n            A tuple containing the refined text and a log dictionary.\n        \"\"\"\n        logging.info(\"Entering refinement phase (Round %d)...\", round_num)\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\n\"\n            f\"Critique:\\n{critique}\\n\\n\"\n            \"Refine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.run_config.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.run_config.full_config,\n            dry_run=self.run_config.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.run_config.refiner_model,\n            \"role_prompt_file_used\": self.run_config.default_refiner_role_prompt_file,\n        }\n        logging.info(\"\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Runs the critique and refine loop.\n\n        Args:\n            initial_user_prompt: The initial prompt from the user.\n            initial_content_for_review: Optional initial content for self-review mode.\n\n        Returns:\n            A tuple containing the final refined output (str) and the run log (dict).\n        \"\"\"\n        self.run_log = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": initial_user_prompt,\n            \"context_documents\": [],  # Placeholder for future RAG\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.run_config.generator_model,\n                \"critic_model\": self.run_config.critic_model,\n                \"refiner_model\": self.run_config.refiner_model,\n                \"max_rounds\": self.run_config.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.run_config.stop_threshold,\n                \"default_critic_role_prompt_file\": self.run_config.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.run_config.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.run_config.multi_critic_roles,\n            },\n        }\n\n        try:\n            current_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n\n            for i in range(self.run_config.max_rounds):\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n                if not meta_critic_template:\n                    logging.warning(\"Meta-critic template not found in config, skipping actionability check.\")\n                else:\n                    actionability_response = await call_model(\n                        prompt=critique_text,\n                        model_name=self.run_config.critic_model,  # Use a fast model for classification\n                        system_prompt=meta_critic_template,\n                        config=self.run_config.full_config,\n                        dry_run=self.run_config.dry_run,\n                        role=\"meta_critic\",\n                    )\n\n                    if not actionability_response or not actionability_response.strip():\n                        logging.warning(\"Meta-critic returned an empty or unexpected response.\")\n                    \n                    if actionability_response.strip() == \"NON_ACTIONABLE\":\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        self.run_log[\"final_output\"] = current_text\n                        break\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.run_config.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = current_text\n\n        except Exception as e:\n            error_message = f\"Error during critique/refine loop: {e}\"\n            logging.error(error_message, exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n",
    "model_used": "N/A (provided content for review)"
  },
  "critiques": [
    {
      "round": 1,
      "text": "This code is well-structured and generally well-written, but several improvements can enhance clarity, efficiency, and robustness.\n\n**Clarity and Conciseness:**\n\n* **`_generate`:** The logic for handling `initial_content_for_review` could be simplified.  Instead of a separate `if` block, directly assign `initial_response` and `initial_generation_log` based on the presence of `initial_content_for_review`.  This avoids code duplication.\n* **`_critique`:** The `if self.run_config.multi_critic_roles` block is quite lengthy. Consider refactoring the common parts into a separate helper function to improve readability.  The exception handling could also be improved (see below).\n* **`_refine`:** The `refine_prompt_for_model` string can be slightly improved by using f-strings more effectively (e.g., combining the newline and the next string).\n* **`run`:** The `self.run_log` initialization is repetitive. Consider creating a dictionary with default values and then updating specific keys within the loop.\n\n**Accuracy and Completeness:**\n\n* **Error Handling:** The `_critique` method's exception handling is insufficient.  Catching `TemplateNotFoundError` and `IOError` separately is redundant; a more general exception type would suffice. Also, the error message is vague; include specific file names if possible.  Consider adding more comprehensive error handling throughout the class, especially around asynchronous operations and model calls.\n* **Logging:** While logging is present, it could be more informative. Include more context in log messages (e.g., specific model parameters used, details about failed calls). Consider using different log levels (e.g., `DEBUG` for more verbose information).\n* **Actionability Check:** The actionability check relies on a string comparison (\"NON_ACTIONABLE\"). This is brittle and prone to errors. It would be much better to have a structured return from the `call_model` function, indicating the actionability with a boolean or similar.\n* **`RunConfig`:** The code heavily relies on `RunConfig`. It's unclear what this class contains and how it manages configuration. More context around the `RunConfig` class and its attributes would be beneficial.\n* **Meta-Critic handling:** The `if not meta_critic_template` block simply logs a warning and continues without the meta-critique.  It might be better to have a configuration setting to disable this feature instead, as the absence of a meta-critic could be intentional.\n\n**Logical Flow and Structure:**\n\n* **`run` function:** The `run` function is quite long.  Consider breaking it down into smaller, more focused functions to improve readability and maintainability.  For instance, separate functions for the initial generation, the iterative critique-refine loop, and the final logging could make the code easier to follow.\n\n**Tone and Style:**\n\n* The docstrings are good but could be more concise in some places.\n\n**Grammar and Spelling Errors:** No errors detected.\n\n\n**Actionable Critique:**\n\n1. **Refactor `_generate` for conciseness:** Combine the conditional logic into a single assignment using a ternary operator or similar.\n2. **Refactor `_critique`:** Create a helper function for the common parts of single and multi-critic logic. Improve exception handling to provide more specific error messages and use a more general exception type.\n3. **Improve `_refine`'s f-string:** Combine the newline and subsequent string.\n4. **Improve error handling:** Add comprehensive error handling throughout the class, particularly around asynchronous operations and model calls.\n5. **Enrich logging:** Add more context and detail to log messages and potentially use different log levels.\n6. **Refactor actionability check:**  Change the string comparison to a structured return from `call_model`.\n7. **Provide `RunConfig` details:** Add more information about the `RunConfig` class and its attributes in the code documentation.\n8. **Handle missing meta-critic gracefully:** Provide a configuration option to disable the meta-critic feature instead of simply continuing without it and logging a warning.\n9. **Refactor `run`:** Break it into smaller, more manageable functions.\n10. **Review and potentially simplify `self.run_log` initialization.**  Use a dictionary with default values to reduce repetition.\n\n\nBy addressing these points, the code will be more robust, readable, and maintainable.  The improved error handling and logging will make debugging significantly easier. The refactoring suggestions will lead to more concise and efficient code.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 2,
      "text": "This code is well-structured and generally well-written, but several improvements can enhance clarity, robustness, and efficiency.\n\n**Clarity and Conciseness:**\n\n* **`_generate` function:** The conditional assignment within `initial_generation_log` could be simplified using a ternary operator:  `initial_generation_log = {\"text\": initial_response, \"model_used\": \"N/A (provided content for review)\" if initial_content_for_review else self.run_config.generator_model}`.\n\n* **`_critique` function:** The `try...except` block within the `if self.run_config.multi_critic_roles:` section could be moved outside the conditional to handle exceptions more generally, improving readability.  The `ValueError` is arguably redundant; the original exception should convey sufficient information.\n\n* **`_check_actionability` function:**  The comment \"Assuming call_model returns a structured response; adapt based on your actual implementation\" highlights a crucial point needing clarification.  The code should explicitly handle different possible return types from `call_model` to prevent unexpected errors.  A more descriptive name like `_is_critique_actionable` might be preferable. The return `True` in the `except` block is questionable. A better approach might be to log the error and raise it or return `False` indicating the inability to determine actionability.\n\n* **`run` function:** The `self.run_log[\"config_used\"].update(...)` block is repetitive and could be streamlined.  Populate `config_used` directly within the `__init__` method.\n\n\n**Accuracy and Completeness:**\n\n* **Error Handling:** While `try...except` blocks are present,  the error handling is somewhat superficial. Consider providing more informative error messages and potentially implementing more sophisticated retry mechanisms for certain operations (e.g., API calls).  The `_check_actionability` function's error handling, as noted above, is especially problematic.\n\n* **Logging:** The logging is good, but could be improved by using different log levels (e.g., `logging.debug` for detailed information) appropriately. Also, consider including timestamps within log messages for better traceability.\n\n* **`_check_actionability` Return Value:** The assumption that `actionability_response.get(\"actionable\", False)` will always return a boolean is risky.  Explicit type checking or validation should be added.\n\n**Logical Flow and Structure:**\n\n* **Configuration:** The way the configuration is handled (`run_config`, `full_config`, etc.) could benefit from a more structured approach. Consider using a dedicated configuration class with data validation.\n\n* **Meta-Critic Logic:**  The logic for the meta-critic is awkwardly placed within the main loop.  It could be extracted into a separate helper function.\n\n\n**Tone and Style:**\n\nThe code's tone and style are generally good.  Docstrings are clear and helpful.\n\n\n**Grammar and Spelling Errors:**\n\nNo errors detected.\n\n\n**Actionable Critique:**\n\n1. **Refactor Configuration:** Create a dedicated configuration class with data validation to improve readability and maintainability.\n\n2. **Improve Error Handling:** Implement more robust error handling, including informative messages and potentially retry mechanisms, especially for API calls and the meta-critic logic.\n\n3. **Streamline Configuration Update:**  Populate `self.run_log[\"config_used\"]` directly within the `__init__` method.\n\n4. **Refactor `_check_actionability`:**  Rename to `_is_critique_actionable`, add explicit type checking for `actionability_response`, and improve error handling (e.g., don't silently return `True` on error).\n\n5. **Improve Logging:** Use more granular log levels and add timestamps to log messages for better debugging.\n\n6. **Extract Meta-Critic Logic:** Move the meta-critic logic into a separate function.\n\n7. **Consolidate Error Handling in `_critique`:** Move the `try...except` block outside the conditional in `_critique`.\n\n\nBy addressing these points, the code will become more robust, maintainable, and easier to understand.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 3,
      "text": "This code is well-structured and mostly well-written, demonstrating a good understanding of asynchronous programming and logging best practices. However, several improvements can enhance clarity, robustness, and efficiency.\n\n**Actionable Critique:**\n\n1. **Error Handling:** While the code includes `try...except` blocks, error handling could be more granular and informative.  Instead of generic `Exception` catches,  catch specific exceptions (e.g., `asyncio.TimeoutError`, `ModelCallError`) and handle them appropriately.  For example, a timeout could trigger a different stop condition in `_run_critique_refine_loop`.  The current error handling masks the root cause, hindering debugging.  Consider adding more specific error messages to the logs, possibly including the exception type and traceback (carefully managing sensitive information).\n\n2. **Logging:** The logging is good, but could be improved.  Consider using different log levels (e.g., `logging.debug`, `logging.warning`) to categorize messages better.  Excessive `logging.info` calls might clutter the logs, especially during normal operation.\n\n3. **`_is_critique_actionable`:** This function's logic is complex and could benefit from refactoring.  The nested conditional checks and error handling make it hard to follow. Consider breaking it down into smaller, more focused functions.  The handling of `actionability_response` lacks robust type checking. A more explicit type hint (e.g. `Dict[str, Any]`) and a check for the existence of the \"actionable\" key before accessing its value would improve reliability. Also, the choice of using the critic model for the meta-critique actionability check might not be ideal;  a specialized smaller, faster model might be more suitable for this task.\n\n4. **Configuration:** The `run_config` is passed around extensively. While this is acceptable, consider whether aspects of it could be moved into the class itself to reduce parameter passing.  This might reduce the complexity and improve readability.\n\n5. **`_generate` function:** The logic for handling `initial_content_for_review` could be simplified. A conditional expression might make the code more concise.\n\n6. **Type Hints:** While type hints are used,  they could be more comprehensive.  For example, the return type of `_critique` could be explicitly defined as `Tuple[str, Dict[str, Any]]`.\n\n\n7. **Asynchronous operations:** The `asyncio.gather` in `_critique` is efficient for parallel calls. However, consider adding a timeout mechanism to prevent indefinite blocking.\n\n\n**Specific Code Examples and Suggestions:**\n\n* **`_generate` function improvement:**\n\n```python\ninitial_response = initial_content_for_review or await call_model(...)\ninitial_generation_log = {\n    \"text\": initial_response,\n    \"model_used\": self.run_config.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n}\n```\n\n* **`_is_critique_actionable` refactoring (example):**\n\n```python\nasync def _is_critique_actionable(self, critique_text: str) -> bool:\n    meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n    if not meta_critic_template:\n        logging.warning(...)  # Log warning\n        return True\n\n    actionability_response = await self._get_meta_critique(critique_text, meta_critic_template) #New Helper Function\n    return actionability_response.get(\"actionable\", False) # safer access\n\nasync def _get_meta_critique(self, critique_text: str, meta_critic_template: str) -> Dict[str, Any]:\n    try:\n        response = await call_model(...) #call to metacritic model.\n        if not isinstance(response, dict):\n            raise ValueError(\"Unexpected response type from meta-critic model.\")\n        return response\n    except (ModelCallError, ValueError) as e:\n        logging.error(f\"Error during meta-critique: {e}\", exc_info=True)\n        return {\"actionable\": False}\n\n```\n\n\nBy addressing these points, the code will be more robust, readable, and maintainable.  The improved error handling will make debugging significantly easier.  The refactoring suggestions will improve code clarity and reduce complexity.\n",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    }
  ],
  "refinements": [
    {
      "round": 1,
      "text": "```python\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple\n# Local imports\nfrom core.model_router import call_model\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],  # Placeholder for future RAG\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": None,\n                \"critic_model\": None,\n                \"refiner_model\": None,\n                \"max_rounds\": None,\n                \"stop_on_no_actionable_critique_threshold\": None,\n                \"default_critic_role_prompt_file\": None,\n                \"default_refiner_role_prompt_file\": None,\n                \"multi_critic_roles\": None,\n                \"disable_meta_critic\": False, #Added configuration option\n            },\n        }\n\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.run_config.generator_model,\n            config=self.run_config.full_config,\n            dry_run=self.run_config.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": \"N/A (provided content for review)\" if initial_content_for_review else self.run_config.generator_model,\n        }\n        logging.info(\"\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.run_config.critic_model,\n                system_prompt=role_template,\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except Exception as e:\n            logging.error(f\"Error generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(\"Entering critique phase (Round %d)...\", round_num)\n        critique_log = {\"round\": round_num, \"model_used\": self.run_config.critic_model}\n\n        if self.run_config.multi_critic_roles:\n            logging.info(\n                \"--- Multi-agent Critique (Roles: %s) ---\",\n                \", \".join(self.run_config.multi_critic_roles),\n            )\n            try:\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.run_config.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.run_config.multi_critic_roles\n            except Exception as e:\n                logging.error(f\"Error during multi-critic phase: {e}\", exc_info=True)\n                raise ValueError(f\"Failed to generate critiques: {e}\") from e\n        else:\n            critique = await self._get_critique(current_text, self.run_config.default_critic_role_prompt_file)\n            critique_log[\"role_prompt_file_used\"] = self.run_config.default_critic_role_prompt_file\n\n        critique_log[\"text\"] = critique\n        logging.info(\"\\n--- Critique ---\\n%s\\n\", critique)\n        return critique, critique_log\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(\"Entering refinement phase (Round %d)...\", round_num)\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.run_config.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.run_config.full_config,\n            dry_run=self.run_config.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.run_config.refiner_model,\n            \"role_prompt_file_used\": self.run_config.default_refiner_role_prompt_file,\n        }\n        logging.info(\"\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.run_config.max_rounds):\n            critique_text, critique_log = await self._critique(current_text, i + 1)\n            self.run_log[\"critiques\"].append(critique_log)\n\n            if not self.run_config.config_used[\"disable_meta_critic\"]: #Check config for meta-critic\n                meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n                if meta_critic_template:\n                    actionability = await self._check_actionability(critique_text, meta_critic_template)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n            refined_response, refinement_log = await self._refine(\n                current_text, critique_text, i + 1\n            )\n            self.run_log[\"refinements\"].append(refinement_log)\n            current_text = refined_response\n        return current_text\n\n    async def _check_actionability(self, critique_text: str, meta_critic_template: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        try:\n            actionability_response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.critic_model,  # Use a fast model for classification\n                system_prompt=meta_critic_template,\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"meta_critic\",\n            )\n            # Assuming call_model returns a structured response; adapt based on your actual implementation\n            return actionability_response.get(\"actionable\", False)  \n        except Exception as e:\n            logging.error(f\"Error during meta-critique actionability check: {e}\", exc_info=True)\n            return True #Assume actionable if meta-critique fails\n\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        self.run_log[\"config_used\"].update({\n            \"generator_model\": self.run_config.generator_model,\n            \"critic_model\": self.run_config.critic_model,\n            \"refiner_model\": self.run_config.refiner_model,\n            \"max_rounds\": self.run_config.max_rounds,\n            \"stop_on_no_actionable_critique_threshold\": self.run_config.stop_threshold,\n            \"default_critic_role_prompt_file\": self.run_config.default_critic_role_prompt_file,\n            \"default_refiner_role_prompt_file\": self.run_config.default_refiner_role_prompt_file,\n            \"multi_critic_roles\": self.run_config.multi_critic_roles,\n            \"disable_meta_critic\": self.run_config.disable_meta_critic, #Added config setting\n        })\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.run_config.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n\n        except Exception as e:\n            error_message = f\"Error during critique/refine loop: {e}\"\n            logging.error(error_message, exc_info=True)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 2,
      "text": "```python\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple, Union\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],  # Placeholder for future RAG\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.run_config.generator_model,\n                \"critic_model\": self.run_config.critic_model,\n                \"refiner_model\": self.run_config.refiner_model,\n                \"max_rounds\": self.run_config.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.run_config.stop_threshold,\n                \"default_critic_role_prompt_file\": self.run_config.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.run_config.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.run_config.multi_critic_roles,\n                \"disable_meta_critic\": self.run_config.disable_meta_critic,\n            },\n        }\n\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.run_config.generator_model,\n            config=self.run_config.full_config,\n            dry_run=self.run_config.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": \"N/A (provided content for review)\" if initial_content_for_review else self.run_config.generator_model,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.run_config.critic_model,\n                system_prompt=role_template,\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except Exception as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.run_config.critic_model}\n\n        try:\n            if self.run_config.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.run_config.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.run_config.multi_critic_roles]\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.run_config.multi_critic_roles\n            else:\n                critique = await self._get_critique(current_text, self.run_config.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.run_config.default_critic_role_prompt_file\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except Exception as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during critique phase: {e}\", exc_info=True)\n            raise\n\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.run_config.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.run_config.full_config,\n            dry_run=self.run_config.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.run_config.refiner_model,\n            \"role_prompt_file_used\": self.run_config.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.run_config.max_rounds):\n            critique_text, critique_log = await self._critique(current_text, i + 1)\n            self.run_log[\"critiques\"].append(critique_log)\n\n            if not self.run_config.config_used[\"disable_meta_critic\"]:\n                actionability = await self._is_critique_actionable(critique_text)\n                if not actionability:\n                    self.run_log[\"reason_for_stopping\"] = (\n                        f\"Non-actionable critique received in round {i + 1}.\"\n                    )\n                    return current_text\n\n            refined_response, refinement_log = await self._refine(\n                current_text, critique_text, i + 1\n            )\n            self.run_log[\"refinements\"].append(refinement_log)\n            current_text = refined_response\n        return current_text\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        if not meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return True\n\n        try:\n            actionability_response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.critic_model,  # Use a fast model for classification\n                system_prompt=meta_critic_template,\n                config=self.run_config.full_config,\n                dry_run=self.run_config.dry_run,\n                role=\"meta_critic\",\n            )\n            # Explicit type checking for actionability_response\n            if isinstance(actionability_response, dict) and \"actionable\" in actionability_response:\n                return bool(actionability_response[\"actionable\"])\n            else:\n                logging.error(f\"[{datetime.now().isoformat()}] Unexpected response from meta-critic model: {actionability_response}\")\n                return False # Handle unexpected response type\n\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique actionability check: {e}\", exc_info=True)\n            return False # Return False if meta-critique call fails\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique actionability check: {e}\")\n            return False\n\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.run_config.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    },
    {
      "round": 3,
      "text": "```python\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],  # Placeholder for future RAG\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles], timeout=60\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            else:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i+1}\"\n                return current_text\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i+1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model if hasattr(self.run_config, 'meta_critic_model') else self.critic_model, #Use specified meta-critic model if available, otherwise fallback to critic model.\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n\n```",
      "model_used": "gemini-1.5-flash",
      "role_prompt_file_used": null
    }
  ],
  "reason_for_stopping": "Max rounds (3) reached.",
  "final_output": "```python\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport time\n\n# Local imports\nfrom core.model_router import call_model, ModelCallError\nfrom core.roles import load_role_template, TemplateNotFoundError\nfrom utils.logger import Logger\nfrom .config import RunConfig\n\n\nclass CritiqueRefineLoop:\n    \"\"\"Encapsulates the critique and refinement loop logic.\"\"\"\n\n    def __init__(self, run_config: RunConfig):\n        \"\"\"Initializes the loop with a run configuration.\n\n        Args:\n            run_config: The configuration for this loop instance.\n        \"\"\"\n        self.run_config = run_config\n        redaction_config = self.run_config.full_config.get(\"redaction_config\", {})\n        self.logger = Logger(\n            log_file_path=self.run_config.log_file_path,\n            redact=self.run_config.redact_logs,\n            keys_to_redact=redaction_config.get(\"keys_to_redact\"),\n            redaction_patterns=redaction_config.get(\"patterns_to_redact\"),\n        )\n        self.generator_model = self.run_config.generator_model\n        self.critic_model = self.run_config.critic_model\n        self.refiner_model = self.run_config.refiner_model\n        self.max_rounds = self.run_config.max_rounds\n        self.stop_threshold = self.run_config.stop_threshold\n        self.default_critic_role_prompt_file = self.run_config.default_critic_role_prompt_file\n        self.default_refiner_role_prompt_file = self.run_config.default_refiner_role_prompt_file\n        self.multi_critic_roles = self.run_config.multi_critic_roles\n        self.disable_meta_critic = self.run_config.disable_meta_critic\n        self.meta_critic_template = self.run_config.roles.get(\"meta_critic_template\")\n        self.full_config = self.run_config.full_config\n        self.dry_run = self.run_config.dry_run\n        self.run_log: Dict[str, Any] = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"original_user_prompt\": None,\n            \"context_documents\": [],  # Placeholder for future RAG\n            \"initial_generation\": {},\n            \"critiques\": [],\n            \"refinements\": [],\n            \"reason_for_stopping\": \"\",\n            \"final_output\": \"\",\n            \"config_used\": {\n                \"generator_model\": self.generator_model,\n                \"critic_model\": self.critic_model,\n                \"refiner_model\": self.refiner_model,\n                \"max_rounds\": self.max_rounds,\n                \"stop_on_no_actionable_critique_threshold\": self.stop_threshold,\n                \"default_critic_role_prompt_file\": self.default_critic_role_prompt_file,\n                \"default_refiner_role_prompt_file\": self.default_refiner_role_prompt_file,\n                \"multi_critic_roles\": self.multi_critic_roles,\n                \"disable_meta_critic\": self.disable_meta_critic,\n            },\n        }\n\n\n    async def _generate(\n        self, initial_user_prompt: str, initial_content_for_review: Optional[str]\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates initial content; uses provided content or calls the generator model.\"\"\"\n        logging.info(\"Entering generation phase...\")\n        initial_response = initial_content_for_review or await call_model(\n            prompt=f\"User prompt: {initial_user_prompt}\",\n            model_name=self.generator_model,\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"generator\",\n        )\n        initial_generation_log = {\n            \"text\": initial_response,\n            \"model_used\": self.generator_model if initial_content_for_review is None else \"N/A (provided content for review)\",\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Initial Response ---\\n%s\\n\", initial_response)\n        return initial_response, initial_generation_log\n\n    async def _get_critique(self, current_text: str, role_file: str) -> str:\n        \"\"\"Helper function to get critique from a single role.\"\"\"\n        try:\n            role_template = load_role_template(role_file)\n            critique = await call_model(\n                prompt=current_text,\n                model_name=self.critic_model,\n                system_prompt=role_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"critic\",\n            )\n            return critique\n        except ModelCallError as e:\n            logging.error(f\"[{datetime.now().isoformat()}] ModelCallError generating critique from {role_file}: {e}\", exc_info=True)\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error generating critique from {role_file}: {e}\")\n            raise\n\n    async def _critique(\n        self, current_text: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Generates a critique; handles single and multi-critic roles.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering critique phase (Round {round_num})...\")\n        critique_log = {\"round\": round_num, \"model_used\": self.critic_model}\n\n        try:\n            if self.multi_critic_roles:\n                logging.info(\n                    f\"[{datetime.now().isoformat()}] --- Multi-agent Critique (Roles: %s) ---\",\n                    \", \".join(self.multi_critic_roles),\n                )\n                critiques = await asyncio.gather(\n                    *[self._get_critique(current_text, role_file) for role_file in self.multi_critic_roles], timeout=60\n                )\n                critique = \"\\n\\n\".join(critiques)\n                critique_log[\"role_prompt_file_used\"] = self.multi_critic_roles\n            else:\n                critique = await self._get_critique(current_text, self.default_critic_role_prompt_file)\n                critique_log[\"role_prompt_file_used\"] = self.default_critic_role_prompt_file\n\n            critique_log[\"text\"] = critique\n            logging.info(f\"[{datetime.now().isoformat()}]\\n--- Critique ---\\n%s\\n\", critique)\n            return critique, critique_log\n        except asyncio.TimeoutError:\n            logging.error(f\"[{datetime.now().isoformat()}] Critique generation timed out.\")\n            raise\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during critique phase: {e}\")\n            raise\n\n\n    async def _refine(\n        self, current_text: str, critique: str, round_num: int\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Refines the text based on the critique.\"\"\"\n        logging.info(f\"[{datetime.now().isoformat()}] Entering refinement phase (Round {round_num})...\")\n        refine_prompt_for_model = (\n            f\"Original text:\\n{current_text}\\n\\nCritique:\\n{critique}\\n\\nRefine the original text based on the critique.\"\n        )\n        refined_response = await call_model(\n            prompt=refine_prompt_for_model,\n            model_name=self.refiner_model,\n            system_prompt=self.run_config.roles.get(\"refiner_template\"),\n            config=self.full_config,\n            dry_run=self.dry_run,\n            role=\"refiner\",\n        )\n        refinement_log = {\n            \"round\": round_num,\n            \"text\": refined_response,\n            \"model_used\": self.refiner_model,\n            \"role_prompt_file_used\": self.default_refiner_role_prompt_file,\n        }\n        logging.info(f\"[{datetime.now().isoformat()}]\\n--- Refined Response ---\\n%s\\n\", refined_response)\n        return refined_response, refinement_log\n\n    async def _run_critique_refine_loop(self, initial_text: str) -> str:\n        \"\"\"Runs the iterative critique and refine loop.\"\"\"\n        current_text = initial_text\n        for i in range(self.max_rounds):\n            try:\n                critique_text, critique_log = await self._critique(current_text, i + 1)\n                self.run_log[\"critiques\"].append(critique_log)\n\n                if not self.disable_meta_critic:\n                    actionability = await self._is_critique_actionable(critique_text)\n                    if not actionability:\n                        self.run_log[\"reason_for_stopping\"] = (\n                            f\"Non-actionable critique received in round {i + 1}.\"\n                        )\n                        return current_text\n\n                refined_response, refinement_log = await self._refine(\n                    current_text, critique_text, i + 1\n                )\n                self.run_log[\"refinements\"].append(refinement_log)\n                current_text = refined_response\n            except asyncio.TimeoutError:\n                self.run_log[\"reason_for_stopping\"] = f\"Timeout in round {i+1}\"\n                return current_text\n            except Exception as e:\n                self.run_log[\"reason_for_stopping\"] = f\"Error in round {i+1}: {e}\"\n                logging.exception(f\"[{datetime.now().isoformat()}] Error in _run_critique_refine_loop: {e}\")\n                return current_text\n        return current_text\n\n    async def _get_meta_critique(self, critique_text: str) -> Dict[str, Any]:\n        \"\"\"Gets meta-critique from the model.\"\"\"\n        if not self.meta_critic_template:\n            logging.warning(f\"[{datetime.now().isoformat()}] Meta-critic template not found. Assuming critique is actionable.\")\n            return {\"actionable\": True}\n        try:\n            response = await call_model(\n                prompt=critique_text,\n                model_name=self.run_config.meta_critic_model if hasattr(self.run_config, 'meta_critic_model') else self.critic_model, #Use specified meta-critic model if available, otherwise fallback to critic model.\n                system_prompt=self.meta_critic_template,\n                config=self.full_config,\n                dry_run=self.dry_run,\n                role=\"meta_critic\",\n            )\n            if not isinstance(response, dict):\n                raise ValueError(f\"Unexpected response type from meta-critic model: {type(response)}\")\n            return response\n        except (ModelCallError, ValueError) as e:\n            logging.error(f\"[{datetime.now().isoformat()}] Error during meta-critique: {e}\", exc_info=True)\n            return {\"actionable\": False}\n        except Exception as e:\n            logging.exception(f\"[{datetime.now().isoformat()}] Unexpected error during meta-critique: {e}\")\n            return {\"actionable\": False}\n\n\n    async def _is_critique_actionable(self, critique_text: str) -> bool:\n        \"\"\"Checks if the critique is actionable using a meta-critic model.\"\"\"\n        meta_critique_result = await self._get_meta_critique(critique_text)\n        return meta_critique_result.get(\"actionable\", False)\n\n\n    async def run(\n        self,\n        initial_user_prompt: str,\n        initial_content_for_review: Optional[str] = None,\n    ) -> Tuple[str, Dict[Any, Any]]:\n        \"\"\"Runs the critique and refine loop.\"\"\"\n        self.run_log[\"original_user_prompt\"] = initial_user_prompt\n        start_time = time.time()\n\n        try:\n            initial_text, initial_generation_log = await self._generate(\n                initial_user_prompt, initial_content_for_review\n            )\n            self.run_log[\"initial_generation\"] = initial_generation_log\n            final_text = await self._run_critique_refine_loop(initial_text)\n            if not self.run_log[\"reason_for_stopping\"]:\n                self.run_log[\"reason_for_stopping\"] = (\n                    f\"Max rounds ({self.max_rounds}) reached.\"\n                )\n            self.run_log[\"final_output\"] = final_text\n            self.run_log[\"runtime\"] = time.time() - start_time\n\n        except Exception as e:\n            error_message = f\"[{datetime.now().isoformat()}] Error during critique/refine loop: {e}\"\n            logging.exception(error_message)\n            self.run_log[\"reason_for_stopping\"] = error_message\n            self.run_log[\"final_output\"] = \"\"\n            raise\n        finally:\n            self.logger.log_run(self.run_log)\n\n        return self.run_log[\"final_output\"], self.run_log\n\n```",
  "config_used": {
    "generator_model": "gemini-1.5-flash",
    "critic_model": "gemini-1.5-flash",
    "refiner_model": "gemini-1.5-flash",
    "max_rounds": 3,
    "stop_on_no_actionable_critique_threshold": 50,
    "default_critic_role_prompt_file": null,
    "default_refiner_role_prompt_file": null,
    "multi_critic_roles": null
  }
}